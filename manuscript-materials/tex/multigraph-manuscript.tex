\documentclass[epjST, final]{svjour}
\usepackage{graphicx, subcaption, setspace, indentfirst, amsfonts, amsthm, empheq}
% \usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
% \usepackage[center]{titlesec}
\graphicspath{ {../figs/} }
% \pagestyle{plain}
% \renewcommand\thesection{\Roman{section}}
% \renewcommand\thesubsection{\alph{subsection}.}
\newcommand\ignore[1]{}
% \setlength{\parskip}{0ex}
% \def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
% \let\endchangemargin=\endlist 

\title{Equation-free analysis of a dynamically evolving multigraph}
\author{Alexander Holiday \inst{1} \and Ioannis G. Kevrekidis
  \inst{1,2}} \institute{Department of Chemical and Biological
  Engineering, Princeton University, Princeton, New Jersey 08544, USA
  \and PACM, Princeton University, Princeton, New Jersey 08544, USA}
% $^1$\textit{Department of Chemical and Biological Engineering},
% \textit{Princeton University, Princeton, New Jersey 08544, USA} \\ \\
% $^2$\textit{Department of Mathematics}, \\
% \textit{The University of British Columbia, Vancouver, British
%   Columbia V6T 1Z4} } \date{}

\abstract{In order to adapt traditional techniques from numerical
  analysis to complex network systems, we apply the equation-free
  framework to a dynamically evolving multigraph system. This
  approach is based on the coupling of short intervals of simulation
  with appropriately-defined lifting and restriction operators that
  map the full network description to suitable macroscopic
  variables, enabling both the acceleration of direct simulations
  through coarse-projective integration, and the identification of
  stationary states via a Newton-GMRES method. Additionally, we
  employ the dimensionality-reduction algorithms principal component
  analysis (PCA) and diffusion maps (DMAPS) to uncover hidden
  structure in the model.}

\begin{document}
\maketitle
\begin{onehalfspace}

\pagebreak

\section{Introduction}
\label{sec:intro}

Over the past decades, myriad systems have been formulated as complex
evolving networks, from highway traffic
\cite{joubert_large-scale_2010} to brain connectivity
\cite{hermundstad_learning_2011}. When modeling such systems, dynamics
are typically defined at a very fine scale, specifying individual node
and edge interactions; explicit closed equations governing macroscopic
properties are often unavailable
\cite{durrett_graph_2012,joubert_large-scale_2010,roche_agent-based_2011,swaminathan_modeling_1998}. These
interactions are often complicated functions dependent on multiple
system parameters, which, when coupled with the large network sizes
inherent in many interesting problems, makes numerical investigation
computationally prohibitive. Additionally, the lack of such explicitly
macroscopic equations precludes the use of traditional numerical
techniques such as optimization and fixed-point finding that could
offer valuable insight into the network's behavior, leaving little
alternative but to work with the full network system. Faced with this
dilemma, investigators must either restrict their attention to a
smaller, heuristically-motivated parameter space
\cite{hodgkin_quantitative_1952} or simplify the model, potentially
removing important features \cite{brown_variability_1999}. \par

Equation-free modeling offers a solution to these obstacles, allowing
one to investigate the complex network at a macroscopic level while
retaining full system details \cite{kevrekidis_equation-free:_2004,gear_equation-free_2003}. Underlying this method is the
assumption that, although we cannot analytically derive equations
governing network evolution, such equations do exist. Furthermore,
these implicit equations are assumed to be functions of only a few
dominating variables. Thus, the state of the complete network is in
fact, by this assumption, well described by these select
quantities. This may seem an onerous restriction, yet it is exactly
the behavior witnessed across network types: despite the complexity of
the full system, certain network properties evolve smoothly in time,
while the evolution of other, secondary properties, is often
correlated to that of a few primary variables
\cite{bold_equation-free_2014,rajendran_coarse_2011,siettos_equation-free_2011}. Once
these significant variables are uncovered, we can combine short
intervals of full system simulation with operators that map the full
system description to and from their representative coarse variables
to perform previously-unfeasible system-level analysis (see Section
\ref{sec:ef} for further details). \par

Here, we apply this framework to a dynamically evolving multigraph model. This offers a test of the methodology in the context of multigraphs, a previously unexplored case. We demonstrate the acceleration of network simulations through coarse-projective integration and the location of stationary states through a matrix-free Newton-GMRES method. In addition, principal component analysis and diffusion maps, two established dimensionality-reduction techniques, are shown to provide an algorithmic method of determining the underlying dimensionality of the system. \par

We begin in Section \ref{sec:m} with a description of the
model. Section \ref{sec:ef} provides further details of the
equation-free method, specifies how it was applied to our system, and
presents subsequent results. Section \ref{sec:dr} summarizes the
operation of PCA and DMAPS, and assesses their effectiveness in
analyzing hidden, low-dimensional structure in this network system.

\section{Model description}
\label{sec:m}

We study the edge-conservative preferential attachment model, a
detailed mathematical analysis of which can be found in
\cite{rath_time_2012} and \cite{rath_multigraph_2012}. The system
evolves in discrete steps $t = 0,1,\ldots t_f$, and we denote the
$n$-vertex graph at each point by $G_n(t)$. The initial state,
$G_n(0)$, is an arbitrary distribution of $m$ edges among the $n$
vertices. No restrictions are placed on this initial distribution:
multiple edges and loops are permitted.\ignore{an Erd\H{o}s-R\'{e}nyi
  random graph on $n$ vertices and $m$ edges. Here, however, $m$ may
  be larger than the total number of edges in a completely-connected
  $n$-vertex graph so we cannot assign a probability to each edges
  existence, instead placing $m$ edges at random in the graph.} The
system is then advanced step-by-step based on the following procedure:

\begin{enumerate}
\item Choose an edge $e_{ij} \in E(G)$ uniformly at random and flip a coin to label one of the ends as $v_{i}$
\item Choose a vertex $v_{k}$ using linear preferential attachment: $P(v_{k} = v_{l}) = \frac{d_{l} + \kappa}{\sum\limits_{i=1}^{n} d_{i} + n \kappa}$
\item Replace $e_{ij}$ with $e_{ik}$
\end{enumerate}

\noindent where $d_i$ is the degree of vertex $v_i$, $E(G)$ is the set of edges in the graph, and $\kappa \in (0, \infty)$ is a model parameter affecting the influence degrees have on the probability distribution. That is, taking $\lim\limits{\kappa \rightarrow 0}$, we recover ``pure'' preferential attachment, and probability scales directly with degree, while $\lim\limits_{\kappa \rightarrow \infty} P(v_k = v_l) = \frac{1}{n} \; \forall \; l$, and individual degrees have no effect. \par

Evolving the system in this manner, the degree sequence approaches a
stationary distribution over $O(n^3)$ steps. As explained in
\cite{rath_time_2012}, this distribution is dependent only on system
parameters $\rho = \frac{2*m}{n}$ and $\kappa$. Fig. (\ref{fig:dse})
illustrates the evolution of the degree sequence of two networks with
different initial configurations but equivalent values of $\rho$ and
$\kappa$; as expected, we observe they approach an identical stationary state.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.9\textwidth]{graph-evo}
  \caption{Schematic of the substeps of the evolution dynamics of the
    multigraph $G(t)$. \label{fig:step-illustration}}
\end{figure}

\begin{figure}[th!]
  \vspace{-5mm}
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{lopsided-degs-a}
    \subcaption{\label{fig:lopsided-init}}
  \end{subfigure} %
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{erdos-degs-a}
    \subcaption{\label{fig:erdos-init}}
  \end{subfigure}
  \caption{Multigraph evolution dynamics for an $n=100$ vertex graph
    observed through the degree distribution. Two distinct transients
    are shown, initialized with (a) an Erd\H{o}s-R\'{e}nyi random
    graph with $m = 5,050$ edges and (b) a graph in which half the
    vertices are isolated, and half completely connected among
    themselves. \label{fig:dse}}
\end{figure}

\begin{figure}[th!]
  % \vspace{-5mm}
  % \centering
  % \begin{subfigure}[t]{0.49\textwidth}
  %   \centering
  %   \includegraphics[width=\textwidth]{triangle-slaving}
  %   \subcaption{\label{fig:sv-n2}}
  % \end{subfigure} %
  % \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{triangle-slaving-n2-n3}
    % \subcaption{\label{fig:sv-n3}}
  % \end{subfigure}
  \caption{Evolution of higher-order statistics (here, the total
    triangle count) for three different network initializations. They
    are quickly ($O(n^2)$ steps) drawn to a slow manifold on which
    they slowly evolve ($O(n^3)$ steps) to a steady
    state. \label{fig:sv}}
\end{figure}


\section{Equation-free modeling}
\label{sec:ef}

\subsection{Coarse-graining}

The first task in the implementation of the equation-free framework is to uncover those few, select variables that suffice to describing the behavior of the full, detailed network. This set of variables should be of lower dimensionality than the full system, and should evolve smoothly. Based on the profiles depicted in Fig. (\ref{fig:dse}), the graph's degree sequence, or equivalently degree distribution, is a good candidate. It progresses smoothly, and captures much information about the underlying network, while providing significant savings in dimensionality from an $O(n^2)$ adjacency matrix to a length-$n$ vector of degrees. \par

However, it is crucial to test whether the evolution of other
properties of the graph can be correlated to degree sequence. If not,
our current description is incomplete and there exist other important
coarse variables that must be accounted for in our macroscopic system
description. To assess this, we constructed graphs with identical
degree sequences but varying triangle counts and observed the
evolution of this property under the dynamics prescribed in Section
\ref{sec:m}. Fig. (\ref{fig:sv}) shows that, within a short time, the
triangle counts are drawn to a shared slow manifold, despite the varied
initial conditions. This suggests that the degree sequence is indeed
an adequate coarse variable for the system. \par

Next we describe the other two key elements to equation-free modeling
which map to and from our microscopic and macroscopic descriptions:
restriction and lifting operators.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=\textwidth]{cpi-diagram}
  \caption{Schematic of our restriction ($R$) and lifting ($L$)
    procedures. The graph's sorted degree sequence is calculated
    ($R_1$) and then fit to a third-order polynomial ($R_2$). This
    maps graphs ($G$) to vectors of four coefficients
    ($\mathbf{c}$). To lift, the polynomial is used to generate a
    degree sequence ($L_1$), which is then used to construct a
    consistent graph ($L_2$). This process maps vectors of
    four coefficients ($\mathbf{c}$) to graphs
    ($G$). $\mathbf{R} = R_2 \circ R_1 $ is our restriction and $\mathbf{L} = L_2 \circ
    L_1$ is our lifting. \label{fig:cpi-diagram}}
\end{figure}


\subsection{Restriction}

The restriction operator translates the full network into its
equivalent coarse variables. Here, this involves mapping a multigraph
to its sorted degree sequence, a simple calculation. However, we can
continue to reduce the dimensionality of our coarse system by keeping
not the length-$n$ degree sequence, but a polynomial fitting of it. To
do so, we first sort the sequence to achieve a smooth, monotonically
increasing dataset, then fit the result to some function. Here, a
third-order polynomial was observed to closely approximate the
sequence throughout time; thus, our coarse description consisted of
the four polynomial coefficients specifying a particular sorted degree
sequence, as shown in Fig. (\ref{fig:cpi-diagram}). The restriction
operator therefore maps multigraphs to length-four coefficient
vectors. This procedure is represented by the blue arrows of
Fig. (\ref{fig:cpi-diagram}).

\subsection{Lifting}

The lifting operator performs the inverse role of restriction: translating coarse variables into full networks. Specifically in the context of our model, we map vectors of polynomial coefficients to full networks in a two-stage process. First, the coefficient vector is used to recreate a sorted degree sequence by evaluating the full polynomial at integer values and rounding to the nearest degree, as depicted in Fig. (\ref{fig:cpi-diagram}). If the sum of the resulting degree sequence is odd, a single degree is added to the largest value to ensure the sequence is graphical. Second, this degree sequence is used as input to a Havel-Hakimi algorithm, creating a graph whose degree sequence matches that of the input \cite{havel_remark_1955,hakimi_realizability_1962}. While the canonical Havel-Hakimi method produces simple graphs, it is not difficult to extend it to multigraphs by allowing the algorithm to wrap around the sequence, producing multiple edges and self-loops. The lifting procedure is represented by the red arrows of Fig. (\ref{fig:cpi-diagram}).

\subsection{Coarse-projective integration (CPI)}
\label{sec:cpi}

The pieces described above are combined to accelerate simulations through coarse-projective integration. Denoting the lifting operator by $\mathbf{L} \; : \; c \rightarrow G$ where $c \in \mathbb{R}^4$ is the vector of polynomial coefficients, and the restriction operator as $\mathbf{R} \; : G \rightarrow c$, the method progresses as follows:

\begin{enumerate}
\item Advance the full system for $t_h$ steps
\label{cpi:heal}
\item Continue for $t_c$ steps, restricting the network at even intervals and saving the resulting coarse variables
\item Using the coarse variables collected over the previous $t_c$ steps, project each variable forward $t_p$ steps with a forward-Euler method
\label{cpi:proj}
\item With the new, projected coarse variables, lift to a full network
\label{cpi:init}
\item Repeat from Step (\ref{cpi:heal}) until desired number of steps have been reached
\end{enumerate}

Note that Step (\ref{cpi:heal}) is necessary to allow the system to approach its slow manifold. Upon initializing a new network in Step (\ref{cpi:init}), certain variables will be far from their expected values in a normal system; however, by hypothesis these quantities quickly evolve to a state in which they are strongly correlated with, or are functions of, the selected coarse variables. Thus, after a short interval of ``healing'', they are drawn to the expected trajectory, after which we begin sampling. This is analogous to Fig. (\ref{fig:sv}). The computational gains arise from the projective step, (\ref{cpi:proj}). Here, we advance the system $t_p$ steps at the cost of one evaluation of forward Euler, instead of $t_p$ direct evaluations. \\

Results of the application of this general framework with the specific lifting and restriction operators previously outlined are shown in Fig. (\ref{fig:cpi-results}). We see good agreement between the CPI-accelerated and normal systems, while reducing the number of detailed steps by one third. It is important to mention that this method was applied not to a single system instantiation, but to a large ensemble. This ensured that when properties such as the fitted polynomial coefficients were averaged over the ensemble they evolved smoothly despite the stochastic nature of the system.

\begin{figure}[h!]
  \vspace{-5mm}
  \centering
  \begin{subfigure}{0.59\textwidth}
    \centering
    \includegraphics[width=\textwidth]{cpi-3d-comp-stretch}
    \subcaption{\label{fig:cpi-error}}
  \end{subfigure} %
  \begin{subfigure}{0.39\textwidth}
    \centering
    \includegraphics[width=\textwidth]{cpi-relative-error-n3}
    \subcaption{\label{fig:self-error}}
  \end{subfigure}%
  \caption{CPI results: (a) the evolution of the CPI-accelerated
    degree sequence (red) compared to direct simulation (blue) and (b)
    error in cpi-accelerated runs calculated by comparing
    cpi-accelerated degree sequences to those arising from an ensemble
    of direct simulations. \label{fig:cpi-results}}
\end{figure}


\section{Newton-GMRES}

Aside from the computational savings of model simulations offered by
CPI, the equation free framework also permits the calculation of
system steady states through matrix-free Newton-GMRES
algorithms. Referring back to the CPI procedure outlined in
Sec. (\ref{sec:cpi}), we can define an operator
$\Theta: d \rightarrow d$ projecting coarse variables at $t$ to their
values at $t + \delta t$: $d(t+\delta t) =\Theta(d(t))$. Note that in
this section, we take the sorted degree sequence as our coarse
variable. A system fixed point could then be located by employing
Newton's method to solve the equation

\begin{align}
\label{eq:f}
F(d) = \Theta(d) - d = 0
\end{align}

However, this requires the repeated solution of
$DF(d^{(k)}) \delta d^{(k+1)} = -F(d^{(j)})$, in which the system
Jacobian, $DF$ is unavailable to us. Thankfully, we may circumvent
this obstacle by estimating the directional derivatives
$DF(d) \d dot \delta d$ via a difference approximation of the form

\begin{align}
  DF(d) \ddot \delta d \approx \frac{\| \delta d \| F(d + h \| d \| \frac{\delta d}{\| \delta d \|}) - F(d)}{h \| d \|}
\end{align}

for nonzero $d$, which in turn is evaluated through calls to $F$ as defined in Eq. (\ref{eq:f})

This is precisely the approach of the Newton-GMRES method, in which the solution to a linear system is searched for in expanding Krylov subspaces \cite{kelley_solving_2003}. Applying this algorithm in conjunction with the $\Theta$ operator defined in Sec. (\ref{sec:cpi}) allowed us to locate the stationary distribution without simply running and observing the full system for long times, as would otherwise be required. Results are shown in Fig. (\ref{fig:newton-results}).

\begin{figure}[h!]
  \vspace{-5mm}
  \centering
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{newton-error}
    \subcaption{\label{fig:newton-error}}
  \end{subfigure} %
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{newton-comp}
    \subcaption{\label{fig:newton-comp}}
  \end{subfigure}%
  \caption{Coarse Newton-GMRES results: (a) evolution of error in the
    coarse Newton-GMRES scheme and (b) visual comparison of the
    algorithm's solution to the stationary state arising from direct
    simulation. \label{fig:newton-results}}
\end{figure}


\section{Algorithmic coarse-graining}
\label{sec:dr}
Crucial to the above analysis was the identification of suitable coarse, system variables: it is the starting point of any equation free method. However, the location of such a low-dimensional description is highly non-trivial. Currently, as in this paper, they are discovered through careful investigation of direct simulations, or are based on previous analytical results. Therefore, any process which could algorithmically guide this search would be of great benefit. We present two such methods below: principal component analysis (PCA) and diffusion maps (DMAPS). First, we review the important topic of defining distances between networks, a prerequisite for any dimensionality-reduction technique.

\subsection{Network distances}

When applying dimensionality reduction techniques to a dataset, it is
necessary to define a distance (or similarity) between each pair of
data point. If these points are a set of vectors in $\mathbb{R}^n$ one
has a number of metrics to choose from, the Euclidean distance being a
common first option. Unfortunately, when individual points are not
vectors but networks, the definition of a computationally feasible
metric becomes far more challenging. Examples such as the maximal
common subgraph and edit distances, defined in \cite{bunke_graph_1998}
and \cite{gao_survey_2010} do define metrics on the space of graphs,
but their computation is $NP\textnormal{-}hard$. Other computationally
feasible approaches include comparing distributions of random
walks on each graph \cite{vishwanathan_graph_2010}, calculating the so-called
$n$-tangle density \cite{gallos_revealing_2014}, or calculating the edit distance
with an approximation algorithm \cite{riesen_approximate_2009,zeng_comparing_2009}.  \\

The strategy used in the following methods, detailed in
\cite{rajendran_analysis_2013} and \cite{xiao_structure-based_2008}
enumerates the number of times a certain set of motifs (or subgraphs)
appears in each network in the dataset. This maps each network to a
vector in $\mathbb{R}^n$, and the Euclidean distance is subsequently
used to calculate the similarity of two graphs. Due to computational
restrictions, we chose to count the number of three- and four-vertex
subgraphs that were contained in each network. As there are eight such
motifs, shown in Fig. (\ref{fig:motifs}), this process $\gamma$ maps
each graph to an eight-dimensional vector:
$\gamma : G \rightarrow \mathbb{R}^8$. Additionally, we applied this
operation to a simplified version of each graph, wherein any
multiple edges were reduced to single edges.

\begin{figure}[h!]
  \vspace{-5mm}
  \centering
  \includegraphics[width=\textwidth]{motifs}
  \caption{List of the subgraphs used to embed each network. The
    number of times each appeared in the simplified input graph was
    calculated, mapping input graphs to $\mathbb{R}^8$. \label{fig:motifs}}
\end{figure}

\subsection{PCA}
\label{sec:pca}
% TODO (referenced fig below)

PCA is used to linearly project data into subspaces that capture the
directions along which the data varies most
\cite{jolliffe_principal_2014}. Given some matrix
$X \in \mathbb{R}^{n \times m}$ in which each of the $m$ column
vectors represent a different collection of the $n$ system variables,
PCA computes a reduced set of $k$ ``principal components''
$z_i \in \mathbb{R}^n$ with $k < n$ that constitute an optimal basis
for the data in the sense that, among linear, $k$-dimensional embeddings, $W = Z^TX$
captures the maximum possible variance in the dataset. \\

This method has found wide application, but can suffer from its
inability to uncover nonlinear relationships among the input
variables. Indeed, many datasets will not lie along some hyperplane
embedded in $\mathbb{R}^n$, but rather will form a nonlinear manifold
throughout the space. \\

Theoretical results in \cite{rath_time_2012} state that, for a given network
size $n$, the final stationary state depends only on the number of
edges present $m$ and the model parameter $\kappa$. To assess both the
validity of our graph embedding technique and the usefulness of the
PCA in the context of complex networks, we generated a dataset of
stationary states over a range of $m$ and $\kappa$ values by directly
running the model over $O(n^3)$ steps ($N$ values of each parameter
were taken, for a total of $N^2$ networks). Each resulting
graph $G(m_i, \kappa_j) = G_{ij}$ was then embedded into
$\mathbb{R}^8$ by counting the number of times each of the subgraphs
shown in Fig. (\ref{fig:motifs}) appeared in the network. Thus
$\gamma(G_{ij}) = v_{ij} \in \mathbb{R}^8$. We then proceeded to
perform a principal component analysis on this collection of vectors
$\{v_{ij}\}_{i,j=1}^N$. Surprisingly, this linear technique succeeded in
uncovering a two-dimensional embedding of the dataset corresponding to
the two underlying parameters $m$ and $\kappa$, as shown in
Fig. (\ref{fig:pca}). This suggests that, given some final network
state $G(m, \kappa)$, by projecting its embedding onto these first two
principal components one could obtain a reasonable approximation of
the hidden parameter $\kappa$.

\begin{figure}[h!]
  \vspace{-5mm}
  \centering
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{pca-2d-rho-a}
    \subcaption{\label{fig:pca-rho}}
  \end{subfigure} %
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{pca-2d-kappa-a}
    \subcaption{\label{fig:pca-kappa}}
  \end{subfigure}%
  \caption{PCA of motif-based embeddings: (a) coloring the
    two-dimensional PCA embedding with $\rho$ and (b) coloring the
    two-dimensional PCA embedding with $\kappa$. \label{fig:pca}}
\end{figure}

\subsection{DMAPS}

Unlike PCA, DMAPS uncovers nonlinear manifolds hidden in data. This is
achieved by solving for the discrete equivalent of the eigenfunctions
and eigenvalues of the Laplace-Beltrami operator over the manifold,
which amounts to calculating leading eigenvector/eigenvalue pairs of a
Markov matrix $A$ describing a diffusion process on the dataset. As the
eigenfunctions of the Laplace-Beltrami operator provide
parameterizations of the underlying domain, the output eigenvectors $\Phi_i$
from DMAPS similarly reveal and parameterize any hidden nonlinear
structure in the input dataset. The $k$-dimensional DMAP of point
$x_i$ is given by

\begin{align*}
  \Psi(x_i; t) = \begin{bmatrix} \lambda_1^t \Phi_1(i) \\ \lambda_2^t
    \Phi_2(i) \\ \vdots \\
    \lambda_k^t \Phi_k(i) \end{bmatrix}
\end{align*}

where $\lambda_i$ is the $i^{th}$ eigenvalue of the Markov matrix $A$,
$\Phi_i(j)$ the $j^{th}$ entry of eigenvector $i$, and the parameter
$t$ allows one to probe multiscale features in the dataset. See
\cite{coifman_diffusion_2006,nadler_diffusion_2006} for
further details. \\

First we applied DMAPS to the dataset described in
Sec. (\ref{sec:pca}). Given the relative success of PCA in this
setting, one would expect DMAPS to also uncover the two-dimensional
embedding corresponding to different values of $m$ and $\kappa$. As
Fig. (\ref{fig:dmaps-rk}) shows, this is indeed the case: using
$\Phi_1$ and $\Phi_4$ to embed the graphs produces a two-dimensional
surface along which both $\rho$ and $\kappa$ vary independently. \\

Additionally, we were interested in embeddings of different
model trajectories. This dataset was generated by sampling two
different model simulations at even intervals as they evolved ($N$
points were sampled from each trajectory). The parameter values $m$
and $\kappa$ were held constant, but one graph was initialized as an
Erd\H{o}s-R\'{e}nyi random graph (Fig. (\ref{fig:erdos-init})), while
the other was initialized as a lopsided graph
(Fig. (\ref{fig:lopsided-init})). Letting $G_e(t)$ refer to the
Erd\H{o}s-R\'{e}nyi-initialized system at step $t$, and $G_l(t)$ the
lopsided-initialized system, the embedding $\gamma$ was used to create
points $\gamma(G_e(t)) = v_e(t) \in \mathbb{R}^8$ and similarly
$\gamma(G_l(t)) = v_l(t) \in \mathbb{R}^8$. \\ 

DMAPS was then applied to this set of $2N$ points, and the
three-dimensional embedding using $\Phi_1$, $\Phi_2$ and $\Phi_3$ is
shown in Fig. (\ref{fig:dmaps-results}). At first, the two points are
mapped to distant regions in $\mathbb{R}^3$ due to their different
initial conditions. They evolve along different coordinates of
the embedding, indicating they traverse two distinct trajectories on
their approach to the stationary state. The particular eigenvector
each trajectory travels along is also significant: as $G_e(t)$ was
mapped to $\Phi_1$ and $G_l(t)$ to $\Phi_2$, we can infer that the
dynamics of the Erd\H{o}s-R\'{e}nyi-initialized system occur more
slowly than those of the lopsided-initialized system. Indeed,
referring back to Fig. (\ref{fig:dse}), we see that the lopsided
network descends more rapidly to the stationary state. Eventually,
their embeddings meet as they arrive at this final, shared state. Thus
DMAPS has proven useful in elucidating both geometric and dynamic
features of the system as shown in Figs. (\ref{fig:dmaps-rk}) and (\ref{fig:dmaps-results}). \\

We see that both PCA and DMAPS, when combined with a suitable
embedding of each graph, uncover useful information pertaining to the
underlying dimensionality of the problem.


\begin{figure}[h!]
  \vspace{-5mm}
  \centering
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{dmaps-3d-convergence-a}
    \subcaption{\label{fig:dmaps-results-regular}}
  \end{subfigure} %
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{dmaps-3d-convergence-zoom-a}
    \subcaption{\label{fig:dmaps-results-zoom}}
  \end{subfigure}%
  \caption{DMAP of motif-based embeddings from two separate simulation
    trajectories: (a) embedding of two trajectories on different
    timescales. While the trajectories are initially embedded in
    separate regions due to their different initial conditions,
    they're eventually mapped close together as they evolve to the
    same coarse stationary state. Final states are circled, and point
    size grows as more steps are taken in each trajectory. (b)
    Enlarged view of the final points in the previous plot, revealing
    the approach to a similar final state. Here, the average of the
    final fifty points is circled. Due to the stochastic nature of
    the system, the final embeddings randomly fluctuate about the
    shared stationary state. \label{fig:dmaps-results}}
\end{figure}

\begin{figure}[h!]
  \vspace{-5mm}
  \centering
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{dmaps-2d-rho-a}
    \subcaption{\label{fig:dmaps-rho}}
  \end{subfigure} %
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{dmaps-2d-kappa-a}
    \subcaption{\label{fig:dmaps-kappa}}
  \end{subfigure}%
  \caption{DMAP of motif-based embeddings from collection of different
    simulations run with different parameter values: (a) coloring the
    two-dimensional DMAPS embedding with $\rho$ and (b) coloring the
    two-dimensional DMAPS embedding with $\kappa$. As with PCA, DMAPS
    uncovered the variables governing the stationary state, known from
    Bal\'{a}zs' paper \label{fig:dmaps-rk}}
\end{figure}


\section{Conclusion}

The equation free framework was successfully applied to the edge-conservative preferential attachment model, accelerating simulations through CPI and locating stationary states through coarse Newton-GMRES. Additionally the underlying two-dimensional description was uncovered via PCA and DMAPS. \\

An open area of investigation is the interpretation of the output from the PCA and DMAPS techniques. As a linear map, PCA retains a clear relationship between its input and output, but is less effective when data is highly nonlinear. DMAPS may perform better the task of dimensionality reduction, but it is unclear what the embedding coordinates relate to in the data.


 
%% This poses several problems to the investigator. First, the interactions themselves may be complicated functions that are expensive to compute and dependent on several network parameters. This, coupled with the large network sizes inherent to many interesting problems makes numerical investigation computationally prohibitive. 

%% For instance, the simple model proposed in \ref{votingmodel} describing the adoption of opinions in a population is defined such that at each simulation step, a random individual is chosen and acted upon.  This precludes the use of traditional numerical techniques such as optimization and fixed-point location that could offer valuable insight into the network's behavior. 
%% We expand 

\begin{acknowledgement}
The authors thank Bal\'{a}zs R\'{a}th for his helpful discussion and
insightful suggestions that have greatly enhanced this work.
\end{acknowledgement}

\end{onehalfspace}

\bibliographystyle{abbrv}
\bibliography{multigraph-refs.bib}

\end{document}
